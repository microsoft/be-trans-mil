# Data preparation for external evaluation on BEST2 dataset

- Copy the dataset to Azure Blob Storage via `azcopy`. See the instructions in [azure_setup.md](azure_setup.md) for how
  to do that.
- Mount the Azure Blob Storage container as a file system on your local workstation or on an Azure VM. Again, instructions
  are in [azure_setup.md](azure_setup.md).
- The following assumes that the storage account is mounted at `/cyted`, and that the BEST2 dataset is in the folder
  `/cyted/best2`. Adjust the paths accordingly if you mounted the storage account at a different location.
- Create subsets of the dataset for H&E and TFF3 slides:

    ```shell
    cp /cyted/best2/*_HE*.svs /cyted/BEST2_HE
    cp /cyted/best2/*_TFF3*.svs /cyted/BEST2_TFF3
    ```

- Create an AzureML dataset from the H&E slides, by going to the AzureML portal / Data / Create data asset. As the name,
  choose `BEST2_HE`. As the data source, choose "From Azure storage", and choose "v1 AzureML API: File" as the type. You
  will be asked to select a datastore (`cyted`) and then pick a file path. Tick the box next to the `BEST2_HE` folder.

## HistoQC

On your VM or compute instance, run HistoQC on the `BEST2_HE` folder. Follow the setup for HistoQC as described in [the
training workflow](he_workflow.md). Then run HistoQC on the `BEST2_HE` folder:

```shell
 python -m histoqc -c ~/HistoQC/histoqc/config/config_v2.1.ini -n 4 "/cyted/BEST2_HE/*.svs" -o /cyted/histoqc_best2_he_slides
 ```

## Extract Bounding Boxes from H&E HistoQC masks

These can be generated by running the following commandline from the repository root. Make sure to adjust the path in
`--masks_dir` to point to the folder where the HistoQC masks are located (the `-o` argument in the HistoQC command
above).

```shell
python cyted/preproc/generate_he_bboxes.py --masks_dir=/cyted/histoqc_best2_he_slides --bbox_json=best2_he_bboxes_from_histoqc_masks.json
```

This will create a JSON file with the bounding boxes, located at the repository root. Note: The file needs to be written
into the folder structure of the repository so that, in the next step, it can be uploaded to Azure alongside the code.

## Adjust the dataset file schema

The folder `BEST2_HE` must contain a file `dataset.tsv` (TAB separated) with at least the following columns:

- `CYT full ID` (the patient identifier)
- `H&E` (the name of the H&E image)
- `TFF3 positive` (`Y` for positive, `N` for negative)
- `Patient pathway` (Strings indicating either surveillance or screening - this will be used for stratifying plots)

Create this file using your favourite spreadsheet editor, and upload it to the `BEST2_HE` folder either via the Azure
portal, or write into the mounted storage account on your compute instance or VM.

## Run the cropping and conversion to TIFF

Adjust the argument `bounding_box_path` to match the path of the JSON file generated in the previous step (`--bbox_json`
argument)

```shell
python cyted/preproc/crop_and_convert_ndpi_to_tiff.py \
  --image_column="H&E" \
  --dataset=BEST2_HE \
  --target_magnifications=10.0 \
  --num_workers=20 \
  --output_dataset=BEST2_ \
  --automatic_output_name=True \
  --display_name=preprocess_best2_he_slides \
  --dataset_csv=dataset.tsv \
  --converted_dataset_csv=dataset.tsv \
  --bounding_box_path=best2_he_bboxes_from_histoqc_masks.json \
  --mask_dataset=histoqc_best2_he_slides \
  --datastore=cyted \
  --docker_shm_size=900g \
  --cluster=A100-x4 \
```

## Running the external evaluation

```shell
python runner.py \
  --model=cyted.TFF3_HECytedMIL \
  --cluster=A100-x4 \
  --mount_in_azureml \
  --model_variant=cyted \
  --strictly_aml_v1 \
  --docker_shm_size=900g \
  --mode=eval_full \
  --azure_datasets=BEST2_preprocessed_he_10.0x \
  --src_checkpoint=<insert_aml_run_id_here> \
  --max_bag_size_inf=0 \
  --intensity_threshold_scale=0.999 \
  --num_top_slides=30
```
